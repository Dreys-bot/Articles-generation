{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861e1d6b-260d-446c-a609-80de6f13ab3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51a9953d-ce9b-41da-bf02-e94366746e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\lib\\site-packages (1.2.2)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.3.2-cp311-cp311-win_amd64.whl (9.2 MB)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\lib\\site-packages (1.10.1)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.11.3-cp311-cp311-win_amd64.whl (44.1 MB)\n",
      "Requirement already satisfied: matplotlib in d:\\anaconda\\lib\\site-packages (3.7.1)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.8.0-cp311-cp311-win_amd64.whl (7.6 MB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17.3 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\anaconda\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\anaconda\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\lib\\site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\anaconda\\lib\\site-packages (from matplotlib) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\anaconda\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\anaconda\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: scipy, scikit-learn, matplotlib\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.1\n",
      "    Uninstalling scipy-1.10.1:\n",
      "      Successfully uninstalled scipy-1.10.1\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.2\n",
      "    Uninstalling scikit-learn-1.2.2:\n",
      "      Successfully uninstalled scikit-learn-1.2.2\n",
      "  Attempting uninstall: matplotlib\n",
      "    Found existing installation: matplotlib 3.7.1\n",
      "    Uninstalling matplotlib-3.7.1:\n",
      "      Successfully uninstalled matplotlib-3.7.1\n",
      "Successfully installed matplotlib-3.8.0 scikit-learn-1.3.2 scipy-1.11.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\anaconda\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: boto3 in d:\\anaconda\\lib\\site-packages (from transformers) (1.24.28)\n",
      "Requirement already satisfied: requests in d:\\anaconda\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: regex in d:\\anaconda\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: sentencepiece in d:\\anaconda\\lib\\site-packages (from transformers) (0.1.99)\n",
      "Requirement already satisfied: sacremoses in d:\\anaconda\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: botocore<1.28.0,>=1.27.28 in d:\\anaconda\\lib\\site-packages (from boto3->transformers) (1.27.59)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in d:\\anaconda\\lib\\site-packages (from boto3->transformers) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in d:\\anaconda\\lib\\site-packages (from boto3->transformers) (0.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: click in d:\\anaconda\\lib\\site-packages (from sacremoses->transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\lib\\site-packages (from sacremoses->transformers) (1.2.0)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in d:\\anaconda\\lib\\site-packages (from botocore<1.28.0,>=1.27.28->boto3->transformers) (2.8.2)\n",
      "Collecting datasets\n",
      "  Using cached datasets-2.14.6-py3-none-any.whl (493 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\anaconda\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in d:\\anaconda\\lib\\site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in d:\\anaconda\\lib\\site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\anaconda\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\anaconda\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.4.1-cp311-cp311-win_amd64.whl (29 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Using cached multiprocess-0.70.15-py311-none-any.whl (135 kB)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in d:\\anaconda\\lib\\site-packages (from datasets) (2023.3.0)\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Collecting huggingface-hub<1.0.0,>=0.14.0 (from datasets)\n",
      "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
      "                                              0.0/302.0 kB ? eta -:--:--\n",
      "     -----                                    41.0/302.0 kB ? eta -:--:--\n",
      "     -----------                             92.2/302.0 kB 1.7 MB/s eta 0:00:01\n",
      "     ---------------------                  174.1/302.0 kB 2.1 MB/s eta 0:00:01\n",
      "     ----------------------------------     276.5/302.0 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------- 302.0/302.0 kB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\lib\\site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\anaconda\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.4)\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.6\n",
      "    Uninstalling dill-0.3.6:\n",
      "      Successfully uninstalled dill-0.3.6\n",
      "Successfully installed datasets-2.14.6 dill-0.3.7 huggingface-hub-0.17.3 multiprocess-0.70.15 xxhash-3.4.1\n",
      "Collecting nlp\n",
      "  Using cached nlp-0.4.0-py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from nlp) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=0.16.0 in d:\\anaconda\\lib\\site-packages (from nlp) (11.0.0)\n",
      "Requirement already satisfied: dill in d:\\anaconda\\lib\\site-packages (from nlp) (0.3.7)\n",
      "Requirement already satisfied: pandas in d:\\anaconda\\lib\\site-packages (from nlp) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\anaconda\\lib\\site-packages (from nlp) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\anaconda\\lib\\site-packages (from nlp) (4.65.0)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from nlp) (3.12.4)\n",
      "Requirement already satisfied: xxhash in d:\\anaconda\\lib\\site-packages (from nlp) (3.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->nlp) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->nlp) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->nlp) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda\\lib\\site-packages (from requests>=2.19.0->nlp) (2023.7.22)\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (from tqdm>=4.27->nlp) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in d:\\anaconda\\lib\\site-packages (from pandas->nlp) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda\\lib\\site-packages (from pandas->nlp) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->nlp) (1.16.0)\n",
      "Installing collected packages: nlp\n",
      "Successfully installed nlp-0.4.0\n",
      "Requirement already satisfied: colorama in d:\\anaconda\\lib\\site-packages (0.4.6)\n",
      "Requirement already satisfied: torch in d:\\anaconda\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in d:\\anaconda\\lib\\site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in d:\\anaconda\\lib\\site-packages (from torch) (4.5.0)\n",
      "Requirement already satisfied: sympy in d:\\anaconda\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in d:\\anaconda\\lib\\site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in d:\\anaconda\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\anaconda\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn scipy matplotlib\n",
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install nlp\n",
    "!pip install colorama\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96cd14e-abc2-4b78-b746-7958254cc3ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfdfe723-f6c6-4968-83d9-dc5b9bc641a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataCollatorForLanguageModeling' from 'transformers' (d:\\anaconda\\Lib\\site-packages\\transformers\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents Personnels\\Projets\\Portfolio project\\LLM_GPT\\LLM_Dialog\\Fine-tuning.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents%20Personnels/Projets/Portfolio%20project/LLM_GPT/LLM_Dialog/Fine-tuning.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mos\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mre\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\u001b[39m,\u001b[39m \u001b[39mjson\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents%20Personnels/Projets/Portfolio%20project/LLM_GPT/LLM_Dialog/Fine-tuning.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents%20Personnels/Projets/Portfolio%20project/LLM_GPT/LLM_Dialog/Fine-tuning.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m DataCollatorForLanguageModeling,  DataCollatorWithPadding, GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, AutoConfig\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Documents%20Personnels/Projets/Portfolio%20project/LLM_GPT/LLM_Dialog/Fine-tuning.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdatasets\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'DataCollatorForLanguageModeling' from 'transformers' (d:\\anaconda\\Lib\\site-packages\\transformers\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import torch, os, re, pandas as pd, json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DataCollatorForLanguageModeling,  DataCollatorWithPadding, GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, AutoConfig\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ba15ec-b0b8-47ef-84c0-e25ffed82bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda\"\n",
    "else:\n",
    "    dev = \"cpu\"\n",
    "device = torch.device(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf212d-8a67-4a29-b9ef-4a5d3ed5ebec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df43d592-725c-45f5-bb4f-0d43d7133bc2",
   "metadata": {},
   "source": [
    "# Model and Tokenization loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce281928-29e2-4be7-8df5-bddded4692cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We load the model\n",
    "base_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "# options: ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f66bdfb-88ab-4194-b59b-9b1c9a7f1b77",
   "metadata": {},
   "source": [
    "## Model and parameters architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f635e8a-5fbb-4fab-a3c0-af77d28a05a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method ModuleUtilsMixin.num_parameters of GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.num_parameters\n",
    "# (wte): Embedding(50262, 768)\n",
    "#     (wpe): Embedding(1024, 768)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a2347b-78b1-4b46-8862-b6920a89d46f",
   "metadata": {},
   "source": [
    "## Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187a7114-98bd-4746-96fb-c29595be3178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # We load the tokenizer\n",
    "base_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "base_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c4963f-58d7-4b6a-88ba-56e87f3fe8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in vocabulary:  50257\n"
     ]
    }
   ],
   "source": [
    "print('Words in vocabulary: ', base_tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ce9199-21a7-4cfe-ba8a-d92b01a6f476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17250"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = base_tokenizer.get_vocab()\n",
    "vocabulary['Hi']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cba872-85f8-4d1f-a1aa-6f025c9c5cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi',\n",
       " ',',\n",
       " 'ĠI',\n",
       " \"'m\",\n",
       " 'ĠVictor',\n",
       " 'Ġand',\n",
       " 'ĠI',\n",
       " 'Ġwork',\n",
       " 'Ġas',\n",
       " 'Ġa',\n",
       " 'ĠData',\n",
       " 'ĠScientist']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hi, I'm Victor and I work as a Data Scientist\"\n",
    "base_tokenizer.tokenize(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3214ca-20bc-4024-b827-15c55194d9e4",
   "metadata": {},
   "source": [
    "And to prepare the text and convert it to the format expected by the model, we would use the encode function, specifying in which format we want it to generate the tensor, Pytorch or TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe89295-cd66-4dc2-ac20-105894ec9d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17250,    11,   314,  1101, 12622,   290,   314,   670,   355,   257,\n",
       "          6060, 33374]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_ids = base_tokenizer.encode(text, return_tensors = 'pt')\n",
    "text_ids\n",
    "\n",
    "# tensorflow\n",
    "#text_ids = base_tokenizer.encode(text, return_tensors = 'tf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d147d-4957-4a40-86b6-998d65589595",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6afb37bd-c95c-4dab-990a-296520bbdfee",
   "metadata": {},
   "source": [
    "## Decoding methods and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d230f0d-2db4-4258-b265-0a8213c7af09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "d:\\Documents Personnels\\Projets\\Portfolio project\\LLM_GPT\\cuda\\Lib\\site-packages\\transformers\\generation\\utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   40,   670,   355,   257,  1366, 11444,   379,   262,  2059,   286,\n",
       "          3442,    11, 14727,    13,   198,   198,     1,    40,  1101,   407]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I work as a data scientist\"\n",
    "text_ids = base_tokenizer.encode(text, return_tensors = 'pt')\n",
    "\n",
    "generated_text_samples = base_model.generate(\n",
    "    text_ids\n",
    ")\n",
    "generated_text_samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a9453c-3d8d-4e8d-aa13-62859cf4b4f5",
   "metadata": {},
   "source": [
    "As the output is again a tensor, we will have to decode the output using the tokenizer token by token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddc0af7-742d-4175-82b9-b5a08e18c522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: I work as a data scientist at the University of California, Berkeley.\n",
      "\n",
      "\"I'm not\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, beam in enumerate(generated_text_samples):\n",
    "    print(f\"{i}: {base_tokenizer.decode(beam, skip_special_tokens=True)}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e46200-f05d-43c4-ba5c-d45c9f8ee386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_text_samples(model, tokenizer, input_text, device, n_samples = 5):\n",
    "    text_ids = tokenizer.encode(input_text, return_tensors = 'pt')\n",
    "    text_ids = text_ids.to(device)\n",
    "    model = model.to(device)\n",
    "\n",
    "    generated_text_samples = model.generate(\n",
    "        text_ids, \n",
    "        max_length= 100,  \n",
    "        num_return_sequences= n_samples,\n",
    "        no_repeat_ngram_size= 2,\n",
    "        repetition_penalty= 1.5,\n",
    "        top_p= 0.92,\n",
    "        temperature= .85,\n",
    "        do_sample= True,\n",
    "        top_k= 125,\n",
    "        early_stopping= True\n",
    "    )\n",
    "    gen_text = []\n",
    "    for t in generated_text_samples:\n",
    "        text = tokenizer.decode(t, skip_special_tokens=True)\n",
    "        gen_text.append(text)\n",
    "\n",
    "        return gen_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e37533-9110-422d-bc8f-f098f4172c2d",
   "metadata": {},
   "source": [
    "# Fine-Tunning for fake news generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758c884-3eb6-41c7-8377-8b1b65d8182e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50260, 768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# special tokens are defined\n",
    "bos = '<|endoftext|>'\n",
    "eos = '<|EOS|>'\n",
    "body = '<|body|>'\n",
    "additional_special_tokens = [body]\n",
    "\n",
    "special_tokens_dict = {'eos_token': eos, 'bos_token': bos, 'pad_token': '<pad>',\n",
    "                       'sep_token': body} \n",
    "                      #  'additional_special_tokens':additional_special_tokens}\n",
    "\n",
    "# the new token is added to the tokenizer\n",
    "num_added_toks = base_tokenizer.add_special_tokens(special_tokens_dict)\n",
    "\n",
    "# model configuration to which we add the special tokens\n",
    "config = AutoConfig.from_pretrained('gpt2', \n",
    "                                    bos_token_id=base_tokenizer.bos_token_id,\n",
    "                                    eos_token_id=base_tokenizer.eos_token_id,\n",
    "                                    pad_token_id=base_tokenizer.pad_token_id,\n",
    "                                    sep_token_id=base_tokenizer.sep_token_id,\n",
    "                                    output_hidden_states=False)\n",
    "\n",
    "# we load the pre-trained model with custom settings\n",
    "base_model = GPT2LMHeadModel.from_pretrained('gpt2', config=config)\n",
    "\n",
    "# model embeding resizing\n",
    "base_model.resize_token_embeddings(len(base_tokenizer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dd6b9b-d057-46a9-9144-4674f1527e3a",
   "metadata": {},
   "source": [
    "## Load and preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c23cbb-aaa6-4953-b34d-611433ca1f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath= './dataset/articles1.csv'\n",
    "# df = pd.read_csv(filepath, encoding = 'utf-8', usecols=['title', 'publication'])\\\n",
    "#                     .rename(columns={'title': 'text'})\n",
    "\n",
    "# pd.set_option(\"display.max_colwidth\", None)\n",
    "# df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6bbbf8-b3ef-490b-ab73-b4001270d9fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\audre\\AppData\\Local\\Temp\\ipykernel_11348\\3635246350.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[title_col] = df.apply(lambda row: remove_publication_headline(row[title_col], row['publication']), axis = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 69768 headlines for training and 7753 for validation\n"
     ]
    }
   ],
   "source": [
    "df = []\n",
    "for filepath in ['./dataset/articles1.csv', './dataset/articles2.csv']:\n",
    "    news_df = pd.read_csv(filepath, encoding = 'utf-8')\n",
    "    df.append(news_df)\n",
    "news_df = pd.concat(df, axis=0)\n",
    "\n",
    "def remove_publication_headline(headline, publication):\n",
    "    # publication col doesn't match exactly with newspaper in title col\n",
    "    if str(publication) in str(headline):\n",
    "        headline = headline.split(' - ')[0]\n",
    "    return headline\n",
    "\n",
    "  \n",
    "def process_headlines_articles(df, title_col, content_col):\n",
    "    # Remove rows with empty or null title or content\n",
    "    titulo_vacio = (df[title_col].str.len() == 0) | df[title_col].isna()\n",
    "    contenido_vacio = (news_df[content_col].str.len() == 0) | news_df[content_col].isna()\n",
    "    df = df[~titulo_vacio & ~contenido_vacio]\n",
    "\n",
    "    # # Remove publication name from title\n",
    "    df[title_col] = df.apply(lambda row: remove_publication_headline(row[title_col], row['publication']), axis = 1)\n",
    "\n",
    "    # # Remove headlines with less than 8 words\n",
    "    titlos_len_ge8 = (df[title_col].str.split().apply(lambda x: len(x)) >= 8)\n",
    "    df = df[titlos_len_ge8]\n",
    "\n",
    "    # # Keep the first 100 words from the content\n",
    "    news_df[content_col] = news_df[content_col].str.split(' ').apply(lambda x: ' '.join(x[:100]))\n",
    "\n",
    "    # # Drop duplicates\n",
    "    text_df = df.drop_duplicates(subset = [title_col, content_col])\\\n",
    "                [[title_col, content_col]]\n",
    "\n",
    "    return text_df\n",
    "\n",
    "# Data cleansing\n",
    "news_df = process_headlines_articles(news_df, title_col='title', content_col='content')\n",
    "\n",
    "# We add the tokens\n",
    "prepare_text = lambda x: ' '.join([bos, x['title'], body, x['content'], eos])\n",
    "news_df['text'] = news_df.apply(prepare_text, axis=1)\n",
    "\n",
    "# Split in train and test\n",
    "df_train_news, df_val_news = train_test_split(news_df, train_size = 0.9, random_state = 77)\n",
    "print(f'There are {len(df_train_news)} headlines for training and {len(df_val_news)} for validation')\n",
    "\n",
    "# we load the datasets from pandas df\n",
    "train_dataset = Dataset.from_pandas(df_train_news[['text']])\n",
    "val_dataset = Dataset.from_pandas(df_val_news[['text']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de5cfdb-e43f-4c64-8a77-13236dfa2bcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', '__index_level_0__'],\n",
       "    num_rows: 69768\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7983ca0-77ac-4c22-9aa7-ee90c51e507c",
   "metadata": {},
   "source": [
    "## Tokenize the train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d81ea29-7338-4f80-92f8-75dd429e1bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = base_tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e1418-9ef3-4986-b6cb-1a3d18b4cdd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def7c8bb-7842-4dd8-bc42-7079cf7b94e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    text = base_tokenizer(examples['text'], truncation=True, max_length=1024)\n",
    "    if len(text) > 1024:\n",
    "        print(len(text))\n",
    "        text = text[:1024]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f66136-b6ce-4cbe-9c3a-6e9f04d62d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf7259bcaa7452faa875a56b54709da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/69768 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# tokenization\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30467e95-f6f2-494c-b448-15cef5d5550f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399340c4f4b1453c8aa43a0956e87ab2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenization\n",
    "tokenized_val_dataset = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a1e91-13e0-4770-bc7d-08d7998f2597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27841017-10b1-48dd-ba2a-1f2cca977edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31e43676-c024-40aa-a1b5-b88b620b7a3c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28be46e4-04d2-4c98-83af-00cd4ac27aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_articles_path = './news-articles_v4'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_articles_path,          # output directory\n",
    "    num_train_epochs=2,              # total # of training epochs\n",
    "    per_device_train_batch_size=5,  # batch size per device during training\n",
    "    per_device_eval_batch_size=32,   # batch size for evaluation\n",
    "    warmup_steps=200,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir=model_articles_path,            # directory for storing logs\n",
    "    prediction_loss_only=True,\n",
    "    save_steps=10000\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=base_tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=base_model,                         # the instantiated  Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_train_dataset,         # training dataset\n",
    "    eval_dataset=tokenized_val_dataset,            # evaluation dataset\n",
    "    \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe99ed4f-1067-49ad-ad22-a9b453e48e36",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Documents Personnels\\Projets\\Portfolio project\\LLM_GPT\\LLM_Dialog\\Fine-tuning.ipynb Cell 40\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Documents%20Personnels/Projets/Portfolio%20project/LLM_GPT/LLM_Dialog/Fine-tuning.ipynb#X54sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a87573-77ff-4152-83b5-7524c64df51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(resume_from_checkpoint=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0524a2f-e0a8-42d0-a266-5df78bef2162",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model()\n",
    "base_tokenizer.save_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff92018f-8289-4803-a1ff-276907682ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trained model loading\n",
    "model_articles_path = './news-articles_v4'\n",
    "\n",
    "news_model = GPT2LMHeadModel.from_pretrained(model_articles_path)\n",
    "news_tokenizer = GPT2Tokenizer.from_pretrained(model_articles_path)\n",
    "\n",
    "bos = news_tokenizer.bos_token\n",
    "eos = news_tokenizer.eos_token\n",
    "sep = news_tokenizer.additional_special_tokens[0]\n",
    "\n",
    "articles = {}\n",
    "headline = \"crisis\"\n",
    "content = generate_n_text_samples(news_model, news_tokenizer, headline, \n",
    "                                      device, n_samples = 1)[0]\n",
    "articles = content.replace(headline, '')\n",
    "\n",
    "for title, content in articles.items():\n",
    "    print('\\033[1m' + title + '\\033[0m')\n",
    "    pretty_print(content)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5939225-585c-477d-920d-a5ce71d7e9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
